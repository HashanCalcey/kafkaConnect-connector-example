#
# Copyright 2018 Confluent Inc.
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# name=gcs-sink
# connector.class=io.confluent.connect.gcs.GcsSinkConnector
# tasks.max=1
# topics=gcs_topic

# gcs.bucket.name=kafkapoc
# gcs.part.size=5242880
# flush.size=3

# gcs.credentials.path=/home/calcey/Downloads/googleServiceKey/My-Project-16b4d96303c4.json

# storage.class=io.confluent.connect.gcs.storage.GcsStorage
# format.class=io.confluent.connect.gcs.format.avro.AvroFormat
# format.class=io.confluent.connect.gcs.format.json.JsonFormat
# partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner

# schema.compatibility=NONE

# Example values when using a time-based partitioner
#partitioner.class=io.confluent.connect.storage.partitioner.TimeBasedPartitioner
# For instance, set partition duration to 1 hour
#partition.duration.ms=3600000
#path.format='year'=YYYY_'month'=MM_'day'=dd_'hour'=HH
#locale=en
#timezone=America/Los_Angeles
# Use a deterministic partitioner based on Kafka record timestamps
#timestamp.extractor=Record
# For example, rotate a file every 15 minutes
#rotate.interval.ms=900000
# Also, set size-based rotation to a high value
#flush.size=1000000

# Or use a field based partitioner
#partitioner.class=io.confluent.connect.storage.partitioner.FieldPartitioner
#partition.field.name=


#############################
# CONFLUENT LICENSE SETTINGS
#############################

# Confluent will issue a license key to each subscriber. The license key will be a short snippet
# of text that you can copy and paste. Without the license key, you can use this connector for a
# 30-day trial period. If you are a subscriber, please contact Confluent Support for more
# information.
# confluent.license=

# Name of the Kafka topic used for Confluent Platform configuration, including licensing
# information.
#confluent.topic=_confluent-command

# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster
# used for licensing. All servers in the cluster will be discovered from the initial connection.
# This list should be in the form <code>host1:port1,host2:port2,...</code>. Since these servers
# are just used for the initial connection to discover the full cluster membership (which may
# change dynamically), this list need not contain the full set of servers (you may want more than
# one, though, in case a server is down).
# This is a required config property
# confluent.topic.bootstrap.servers=localhost:9092

# The replication factor for the Kafka topic used for Confluent Platform configuration, including
# licensing information. This is used only if the topic does not already exist, and the default
# of 3 is appropriate for production use. If you are using a development environment with less
# than 3 brokers, you must set this to the number of brokers (often 1).
#confluent.topic.replication.factor=3
# confluent.topic.replication.factor=1

# In secured clusters, add any security configs to the Kafka clients used on confluent.topic by 
# using the prefixes, such as confluent.topic. 
# or confluent.topic.producer. and confluent.topic.consumer. 
# if settings differ between the producer and the consumer that read the confluent.topic

### Standard connector configuration

## Fill in your values in these:

# Unique name for the connector.
# Attempting to register again with the same name will fail.
name=my-gcs-connector

## These must have exactly these values:

# The Java class for the connector
connector.class=io.aiven.kafka.connect.gcs.GcsSinkConnector

# The key converter for this connector
# (must be set to ByteArrayConverter)
key.converter=org.apache.kafka.connect.converters.ByteArrayConverter

# The value converter for this connector
# (must be set to ByteArrayConverter)
value.converter=org.apache.kafka.connect.converters.ByteArrayConverter

# A comma-separated list of topics to use as input for this connector
# Also a regular expression version `topics.regex` is supported.
# See https://kafka.apache.org/documentation/#connect_configuring
topics=server1.dbo.students


### Connector-specific configuration
### Fill in you values

# The name of the GCS bucket to use
# Required.
# gcs.bucket.name=my-gcs-bucket
gcs.bucket.name=kafkapoc
# GCP credentials as a JSON object.
# Required.
# Note that the connector supports passing GCP credentials
# as a file, but this is not supported on Aiven platform.

# gcs.credentials.path=/home/calcey/Downloads/googleServiceKey/My-Project-16b4d96303c4.json

# The set of the fields that are to be output, comma separated.
# Supported values are: `key`, `value`, `offset`, and `timestamp`.
# Optional, the default is `value`.
format.output.fields=key,value,offset,timestamp

# The prefix to be added to the name of each file put on GCS.
# See the GCS naming requirements https://cloud.google.com/storage/docs/naming
# Optional, the default is empty.
file.name.prefix=some-prefix/

# The compression type used for files put on GCS.
# The supported values are: `gzip`, `none`.
# Optional, the default is `none`.
file.compression.type=gzip
